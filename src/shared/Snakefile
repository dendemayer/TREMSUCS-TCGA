configfile: "config.yaml"
import glob
import os
import pandas as pd
script_path = workflow.basedir


rule download_helpfiles:
    """
    this rule will download the gdc_manifest and the gtf annotation file,
    md5checksum checks are performed
    """
    output:
        "{output_path}/metadata/{output_file}"
    conda:
        "envs/request.yaml"
    log:
        "{output_path}/logs/download_helpfiles/{output_file}.gz.log"
    script:
        "scripts/download_help_files.py"

#rule unzip_meta_files:
#    """
#    this is just necessary to overcome the problem of DAG resolving..
#    """
#    input:
#        "{output_path}/metadata/{output_file}.gz"
#    output:
#        "{output_path}/metadata/{output_file}"
#    conda:
#        "envs/gzip.yaml"
#    log:
#        "{output_path}/logs/download_helpfiles/{output_file}.log"
#    shell:
#        "gzip -dk {input}"

rule download_aux_files:
    """
    this rule loads the aux files for both pipelines:
    those are the 
    nationwidechildrens.org....
    files
    nationwidechildrens.org_auxiliary_cesc.txt
    # the same scrip like in download_data_files rule can be used, the output
    # path is different
    """
    input:
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file'])
    output:
        "{output_path}/{project}/aux_files/{output_file}"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{project}/logs/download_aux_files/{output_file}.log"
    script:
        "scripts/download_data_files.py"

rule download_data_files:
    """
    datafiles loaded via script
    """
    input:
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file'])
    output:
        "{output_path}/{project}/{pipeline}/data_files/{output_file}"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{project}/{pipeline}/logs/download_data_files/{output_file}.log"
    script:
        "scripts/download_data_files.py"

def get_aliquot_table(wildcards):
    proj_suffix = wildcards[1].split('-')[1].lower()
    OUTPUT_PATH = wildcards[0]
    project = wildcards[1]
    #pipeline = wildcards[2] # not needed in input
    suppl_2 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_biospecimen_aliquot_{proj_suffix}.txt'
    suppl_3 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_clinical_drug_{proj_suffix}.txt'
    suppl_4 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_clinical_patient_{proj_suffix}.txt'
    # IMPORTANT they are not working clinical tables in the repo, (just
    # concerning CESC) ,the right ones are those:
    # nationwidechildrens.org_clinical_patient_cesc.txt_1
    # nationwidechildrens.org_clinical_follow_up_v4.0_cesc.txt_1 for now just
    # overwrite the first loaded file with the new one (that are those where 2
    # diff filenames are given for one specific uuid
    DF_temp = pd.read_table(os.path.join(OUTPUT_PATH, 'metadata', config['manifest_file'][0]))
    DF_temp = DF_temp[DF_temp['filename'].str.contains(f'nationwidechildrens.org_clinical_follow_up_v.*_{proj_suffix}.txt')]['filename']
    follow_up_table = DF_temp[~DF_temp.str.contains('nte')].sort_values(ascending=False).iloc[0]
    suppl_5 = f'{OUTPUT_PATH}/{project}/aux_files/{follow_up_table}'
    suppl_6 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_biospecimen_sample_{proj_suffix}.txt'
    final_list = [suppl_2, suppl_3, suppl_4, suppl_5, suppl_6]
    return final_list

rule merge_meta_tables:
    """
    merging meta files to connect case id with the datafiles
    """
    input:
        expand("{script_path}/resources/GCv36_Manifests/{{project}}.tsv", script_path=script_path),
        get_aliquot_table,  # input[1-5]
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file']), # input[0]
        "scripts/create_merged_tables.py",
    output:
        #"{output_path}/{project}/{pipeline}/merged_meta_files/{cutoff}/merged_meta_tables.tsv",
        "{output_path}/{project}/{pipeline}/merged_meta_files/{cutoff}/meta_info_druglist_merged_drugs_combined.tsv"
    conda:
        "envs/pandas.yaml"
    wildcard_constraints: # important to just match singleproject patterns
        project = "[A-Z]*-[A-Z]*"
    log:
        "{output_path}/{project}/{pipeline}/logs/merge_meta_tables/{cutoff}.log"
    script:
        "scripts/create_merged_tables.py"

def get_multi_proj_drug_merge_tables(wildcards):
    """
    # to create the aggregated meta table over all projects, return the single project meta tables:
    wildcard structure like:
    ['/scr/dings/PEVO/NEW_downloads_3/TCGA-pipelines', 'TCGA-CESC_TCGA-HNSC',
    'carboplatn,paclitaxel_cisplatin', 'male', 'cutoff_0', 'threshold_0']
    we need every single proj merged table:
    "{output_path}/{project}/metilene/merged_meta_files/meta_info_druglist_merged_drugs_combined.tsv"
    """
    final_list = []
    projects = wildcards[1].split('_')
    for project in projects:
        final_list.append(
                os.path.join(wildcards[0], project, wildcards[2], 'merged_meta_files', wildcards[3], 'meta_info_druglist_merged_drugs_combined.tsv')) 
    return final_list

rule merge_meta_tables_multiproj:
    """
    take the single project meta_info_druglist_merged_drugs_combined.tsv  and
    concat them in multiproj dirs
    """
    input:
        get_multi_proj_drug_merge_tables
    output:
        "{output_path}/{projects}/{pipeline}/merged_meta_files/{cutoff}/meta_info_druglist_merged_drugs_combined.tsv"
    wildcard_constraints: # make clear, that we want to include here multiproj
        projects = "[A-Z]*-[A-Z]*_.*"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{projects}/{pipeline}/logs/merge_meta_tables_multiproj/{cutoff}.log"
    script:
        "scripts/create_merged_tables_multi.py"
    # not possible to invoke the conda env with the run statement
    #run:
    #    import pandas as pd
    #    pd.concat([pd.read_table(i) for i in input]).to_csv(output[0], sep='\t', index=False)
