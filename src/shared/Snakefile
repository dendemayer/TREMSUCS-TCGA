#configfile: "config.yaml"
import glob
import os
import pandas as pd

threshold_str = config['thresh']

script_path = workflow.basedir
#script_path = snakemake.workflow.script_path
# the script path will change depending on the main module we have acces from
# to it, but the scripts invoked here are always withing the shared dir, so go
# one dir up an reenter into shared (eiterh from shared back into shared or
# metilene into shrared or deseq into shared:
script_path = os.path.join(script_path, os.path.pardir, 'shared')


rule download_helpfiles:
    """
    this rule will download the gdc_manifest and the gtf annotation file,
    md5checksum checks are performed
    """
    output:
        "{output_path}/metadata/{output_file}"
    conda:
        "envs/request.yaml"
    log:
        "{output_path}/logs/download_helpfiles/{output_file}.log"
    script:
        "scripts/download_help_files.py"

rule edit_annotation:
    """
    out of the provided annotation gtf, filter out the gene annotations with chr, start, ENSG, gene_type, gene_status, gene_name
    """
    input:
        annot_input = expand("{{output_path}}/metadata/{gtf_gz}", gtf_gz=config['gtf.gz']),
        script_file = os.path.join(script_path, "scripts/edit_annotation.py")
    output:
        annot_output = expand("{{output_path}}/metadata_processed/{gtf}_genes_transcripts.gz", gtf=config['gtf'])
    log:
        expand("{{output_path}}/logs/edit_annotation/{gtf}_annotated_genes.log", gtf=config['gtf'])
    conda:
        "envs/pandas.yaml"
    script:
        "scripts/edit_annotation.py"

#rule unzip_meta_files:
#    """
#    this is just necessary to overcome the problem of DAG resolving..
#    """
#    input:
#        "{output_path}/metadata/{output_file}.gz"
#    output:
#        "{output_path}/metadata/{output_file}"
#    conda:
#        "envs/gzip.yaml"
#    log:
#        "{output_path}/logs/download_helpfiles/{output_file}.log"
#    shell:
#        "gzip -dk {input}"

rule download_aux_files:
    """
    this rule loads the aux files for both pipelines:
    those are the 
    nationwidechildrens.org....
    files
    nationwidechildrens.org_auxiliary_cesc.txt
    # the same scrip like in download_data_files rule can be used, the output
    # path is different
    """
    input:
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file'])
    output:
        "{output_path}/{project}/aux_files/{output_file}"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{project}/logs/download_aux_files/{output_file}.log"
    script:
        "scripts/download_data_files.py"

rule download_data_files:
    """
    datafiles loaded via script
    """
    input:
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file'])
    output:
        "{output_path}/{project}/{pipeline}/data_files/{output_file}"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{project}/{pipeline}/logs/download_data_files/{output_file}.log"
    script:
        "scripts/download_data_files.py"

def get_aliquot_table(wildcards):
    proj_suffix = wildcards[1].split('-')[1].lower()
    OUTPUT_PATH = wildcards[0]
    project = wildcards[1]
    #pipeline = wildcards[2] # not needed in input
    suppl_2 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_biospecimen_aliquot_{proj_suffix}.txt'
    suppl_3 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_clinical_drug_{proj_suffix}.txt'
    suppl_4 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_clinical_patient_{proj_suffix}.txt'
    # IMPORTANT they are not working clinical tables in the repo, (just
    # concerning CESC) ,the right ones are those:
    # nationwidechildrens.org_clinical_patient_cesc.txt_1
    # nationwidechildrens.org_clinical_follow_up_v4.0_cesc.txt_1 for now just
    # overwrite the first loaded file with the new one (that are those where 2
    # diff filenames are given for one specific uuid
    DF_temp = pd.read_table(os.path.join(OUTPUT_PATH, 'metadata', config['manifest_file'][0]))
    DF_temp = DF_temp[DF_temp['filename'].str.contains(f'nationwidechildrens.org_clinical_follow_up_v.*_{proj_suffix}.txt')]['filename']
    follow_up_table = DF_temp[~DF_temp.str.contains('nte')].sort_values(ascending=False).iloc[0]
    suppl_5 = f'{OUTPUT_PATH}/{project}/aux_files/{follow_up_table}'
    suppl_6 = f'{OUTPUT_PATH}/{project}/aux_files/nationwidechildrens.org_biospecimen_sample_{proj_suffix}.txt'
    final_list = [suppl_2, suppl_3, suppl_4, suppl_5, suppl_6]
    return final_list

rule merge_meta_tables:
    """
    merging meta files to connect case id with the datafiles
    here is also the cutoff applied
    """
    input:
        expand("{script_path}/resources/GCv36_Manifests/{{project}}.tsv", script_path=script_path),
        get_aliquot_table,  # input[1-5]
        expand("{{output_path}}/metadata/{manifest}", manifest=config['manifest_file']), # input[0]
        os.path.join(script_path, "scripts/create_merged_tables.py"),
    output:
        #"{output_path}/{project}/{pipeline}/merged_meta_files/{cutoff}/merged_meta_tables.tsv",
        "{output_path}/{project}/{pipeline}/merged_meta_files/{cutoff}/meta_info_druglist_merged_drugs_combined.tsv"
    conda:
        "envs/pandas.yaml"
    wildcard_constraints: # important to just match singleproject patterns
        project = "[A-Z]*-[A-Z]*"
    log:
        "{output_path}/{project}/{pipeline}/logs/merge_meta_tables/{cutoff}.log"
    script:
        "scripts/create_merged_tables.py"

def get_multi_proj_drug_merge_tables(wildcards):
    """
    # to create the aggregated meta table over all projects, return the single project meta tables:
    wildcard structure like:
    ['/scr/dings/PEVO/NEW_downloads_3/TCGA-pipelines', 'TCGA-CESC_TCGA-HNSC',
    'carboplatn,paclitaxel_cisplatin', 'male', 'cutoff_0', 'threshold_0']
    we need every single proj merged table:
    "{output_path}/{project}/metilene/merged_meta_files/meta_info_druglist_merged_drugs_combined.tsv"
    """
    final_list = []
    projects = wildcards[1].split('_')
    for project in projects:
        final_list.append(
                os.path.join(wildcards[0], project, wildcards[2], 'merged_meta_files', wildcards[3], 'meta_info_druglist_merged_drugs_combined.tsv')) 
    return final_list

rule merge_meta_tables_multiproj:
    """
    take the single project meta_info_druglist_merged_drugs_combined.tsv  and
    concat them in multiproj dirs
    """
    input:
        get_multi_proj_drug_merge_tables
    output:
        "{output_path}/{projects}/{pipeline}/merged_meta_files/{cutoff}/meta_info_druglist_merged_drugs_combined.tsv"
    wildcard_constraints: # make clear, that we want to include here multiproj
        projects = "[A-Z]*-[A-Z]*_.*"
    conda:
        "envs/pandas.yaml"
    log:
        "{output_path}/{projects}/{pipeline}/logs/merge_meta_tables_multiproj/{cutoff}.log"
    script:
        "scripts/create_merged_tables_multi.py"
    # not possible to invoke the conda env with the run statement
    #run:
    #    import pandas as pd
    #    pd.concat([pd.read_table(i) for i in input]).to_csv(output[0], sep='\t', index=False)


rule plot_diffs:
    """
    for every found ensg or start of a DMR, plot the pval or the life mean
    diff, also together, if applied, with the different thresholds invoked
    """
    input:
        lifeline_aggregated = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_lifelines_aggregated.tsv.gz", threshold_str=threshold_str),
        script_file = os.path.join(script_path, "scripts/plot_thresh_diff.py")
    output:
        plot_diffs = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_plot_diffs_{{plot_type}}-{{count_type}}.pdf", threshold_str=threshold_str),
        plot_diffs_table = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_plot_diffs_{{plot_type}}-{{count_type}}.tsv.gz", threshold_str=threshold_str),
    conda:
        "envs/seab_matplot_plotl.yaml"
    threads:
        1
    log:
        "{output_path}/{project}/{pipeline}/logs/plot_diffs/{drug_combi}_{gender}_{cutoff}_{plot_type}_{count_type}.log",
    script:
        "scripts/plot_thresh_diff.py"

rule plot_diffs_eval:
    """
    for every found ensg or start of a DMR, plot the pval or the life mean
    diff, also together, if applied, with the different thresholds invoked
    -> just for the evaluated features (scored ones)
    -> the evaluated tables are also called lifeline_aggregated in the input to
    keep reusability of the same script
    """
    input:
        lifeline_aggregated = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_lifelines_evaluated-{{count_type}}.tsv.gz", threshold_str=threshold_str),
        script_file = os.path.join(script_path, "scripts/plot_thresh_diff.py")
    output:
        plot_diffs = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_plot_eval_diffs_{{plot_type}}-{{count_type}}.pdf", threshold_str=threshold_str),
        plot_diffs_table = expand("{{output_path}}/{{project}}/{{pipeline}}/{{pipeline}}_output/{{drug_combi}}/{{gender}}/{{cutoff}}/{threshold_str}/{{pipeline}}_plot_eval_diffs_{{plot_type}}-{{count_type}}.tsv.gz", threshold_str=threshold_str),
    conda:
        "envs/seab_matplot_plotl.yaml"
    threads:
        1
    log:
        "{output_path}/{project}/{pipeline}/logs/plot_diffs_eval/{drug_combi}_{gender}_{cutoff}_{plot_type}_{count_type}.log",
    script:
        "scripts/plot_thresh_diff.py"
